# quack-core/tests/test_integrations/llms/test_models.py
"""
Tests for LLM data models.

This module tests the Pydantic models used in the LLM integration, ensuring proper
validation, conversion, and default values.
"""

import pytest
from pydantic import ValidationError

from quack_core.integrations.llms.models import (
    ChatMessage,
    FunctionCall,
    FunctionDefinition,
    FunctionParameter,
    LLMOptions,
    LLMResult,
    RoleType,
    ToolCall,
    ToolDefinition,
)


class TestLLMModels:
    """Tests for LLM data models."""

    def test_role_type_enum(self) -> None:
        """Test the RoleType enumeration."""
        assert RoleType.SYSTEM.value == "system"
        assert RoleType.USER.value == "user"
        assert RoleType.ASSISTANT.value == "assistant"
        assert RoleType.FUNCTION.value == "function"
        assert RoleType.TOOL.value == "tool"

    def test_chat_message(self) -> None:
        """Test the ChatMessage model."""
        # Test minimal message
        message = ChatMessage(role=RoleType.USER, content="Hello")
        assert message.role == RoleType.USER
        assert message.content == "Hello"
        assert message.name is None
        assert message.function_call is None
        assert message.tool_calls is None

        # Test complete message
        message = ChatMessage(
            role=RoleType.ASSISTANT,
            content=None,
            name="assistant_name",
            function_call={"name": "test_function", "arguments": "{}"},
            tool_calls=[
                {
                    "id": "call_1",
                    "type": "function",
                    "function": {"name": "test_func", "arguments": "{}"},
                }
            ],
        )
        assert message.role == RoleType.ASSISTANT
        assert message.content is None
        assert message.name == "assistant_name"
        assert message.function_call == {"name": "test_function", "arguments": "{}"}
        assert (
            message.tool_calls is not None
        )  # Ensure tool_calls is not None before checking length
        assert len(message.tool_calls) == 1
        assert message.tool_calls[0]["id"] == "call_1"

        # Test from_dict with minimal data
        message = ChatMessage.from_dict({"role": "user", "content": "Hello"})
        assert message.role == RoleType.USER
        assert message.content == "Hello"

        # Test from_dict with complete data
        message = ChatMessage.from_dict(
            {
                "role": "assistant",
                "content": None,
                "name": "assistant_name",
                "function_call": {"name": "test_function", "arguments": "{}"},
                "tool_calls": [
                    {
                        "id": "call_1",
                        "type": "function",
                        "function": {"name": "test_func", "arguments": "{}"},
                    }
                ],
            }
        )
        assert message.role == RoleType.ASSISTANT
        assert message.content is None
        assert message.name == "assistant_name"
        assert message.function_call == {"name": "test_function", "arguments": "{}"}
        assert message.tool_calls is not None  # Ensure tool_calls is not None
        assert len(message.tool_calls) == 1

        # Test from_dict with default role
        message = ChatMessage.from_dict({"content": "Hello"})
        assert message.role == RoleType.USER
        assert message.content == "Hello"

        # Test validation - role is required
        with pytest.raises(ValidationError):
            # Using empty dict to test validation error instead of empty constructor
            ChatMessage(**{})  # This will raise ValidationError since role is required

    def test_function_parameter(self) -> None:
        """Test the FunctionParameter model."""
        # Test minimal parameters
        param = FunctionParameter(name="test", type="string")
        assert param.name == "test"
        assert param.type == "string"
        assert param.description is None
        assert param.required is False

        # Test complete parameters
        param = FunctionParameter(
            name="test",
            type="string",
            description="A test parameter",
            required=True,
        )
        assert param.name == "test"
        assert param.type == "string"
        assert param.description == "A test parameter"
        assert param.required is True

        # Test validation
        with pytest.raises(ValidationError):
            FunctionParameter(**{"type": "string"})  # Missing name

        with pytest.raises(ValidationError):
            FunctionParameter(**{"name": "test"})  # Missing type

    def test_function_definition(self) -> None:
        """Test the FunctionDefinition model."""
        # Test minimal function definition
        func = FunctionDefinition(name="test_function")
        assert func.name == "test_function"
        assert func.description is None
        assert func.parameters == {}

        # Test complete function definition
        func = FunctionDefinition(
            name="test_function",
            description="A test function",
            parameters={
                "param1": {"type": "string", "description": "Parameter 1"},
                "param2": {"type": "integer", "required": True},
            },
        )
        assert func.name == "test_function"
        assert func.description == "A test function"
        assert "param1" in func.parameters
        assert "param2" in func.parameters
        assert func.parameters["param1"]["type"] == "string"
        assert func.parameters["param2"]["required"] is True

        # Test validation
        with pytest.raises(ValidationError):
            FunctionDefinition(**{})  # Missing name

    def test_tool_definition(self) -> None:
        """Test the ToolDefinition model."""
        # Test minimal tool definition
        tool = ToolDefinition(function=FunctionDefinition(name="test_function"))
        assert tool.type == "function"  # Default value
        assert tool.function.name == "test_function"

        # Test complete tool definition
        tool = ToolDefinition(
            type="function",
            function=FunctionDefinition(
                name="test_function",
                description="A test function",
                parameters={"param1": {"type": "string"}},
            ),
        )
        assert tool.type == "function"
        assert tool.function.name == "test_function"
        assert tool.function.description == "A test function"
        assert "param1" in tool.function.parameters

        # Test validation
        with pytest.raises(ValidationError):
            ToolDefinition(**{})  # Missing function

    def test_function_call(self) -> None:
        """Test the FunctionCall model."""
        # Test minimal function call
        call = FunctionCall(name="test_function", arguments="{}")
        assert call.name == "test_function"
        assert call.arguments == "{}"

        # Test with complex arguments
        call = FunctionCall(
            name="test_function",
            arguments='{"param1": "value1", "param2": 42}',
        )
        assert call.name == "test_function"
        assert call.arguments == '{"param1": "value1", "param2": 42}'

        # Test validation
        with pytest.raises(ValidationError):
            FunctionCall(**{"arguments": "{}"})  # Missing name

        with pytest.raises(ValidationError):
            FunctionCall(**{"name": "test_function"})  # Missing arguments

    def test_tool_call(self) -> None:
        """Test the ToolCall model."""
        # Test valid call creation
        call = ToolCall(
            id="call_1",
            type="function",
            function=FunctionCall(name="test_function", arguments="{}"),
        )
        assert call.id == "call_1"
        assert call.type == "function"
        assert call.function.name == "test_function"

        # Test without the required function field
        with pytest.raises(ValidationError):
            ToolCall(
                id="call_1",
                type="function",
                # Missing function field
            )

        # If the model is configured to allow extra fields, our test should reflect that
        # Let's check if the model actually has forbid extra fields enabled
        # If it doesn't, this test doesn't make sense
        try:
            model_config = ToolCall.model_config
            extra_forbidden = model_config.get("extra") == "forbid"
        except (AttributeError, KeyError):
            extra_forbidden = False

        if extra_forbidden:
            with pytest.raises(ValidationError):
                ToolCall(
                    id="call_1",
                    type="function",
                    function=FunctionCall(name="test_function", arguments="{}"),
                    extra_field="should not be allowed",
                )

    def test_llm_options(self) -> None:
        """Test the LLMOptions model."""
        # Test with default options
        options = LLMOptions()
        assert options.temperature == 0.7
        assert options.max_tokens is None
        assert options.top_p == 1.0
        assert options.frequency_penalty == 0.0
        assert options.presence_penalty == 0.0
        assert options.stop is None
        assert options.functions is None
        assert options.tools is None
        assert options.model is None
        assert options.response_format is None
        assert options.seed is None
        assert options.stream is False
        assert options.timeout == 60
        assert options.retry_count == 3
        assert options.initial_retry_delay == 1.0
        assert options.max_retry_delay == 30.0

        # Test with custom options
        custom_function = FunctionDefinition(name="test_function")
        custom_tool = ToolDefinition(function=FunctionDefinition(name="test_function"))

        options = LLMOptions(
            temperature=0.5,
            max_tokens=100,
            top_p=0.9,
            frequency_penalty=0.2,
            presence_penalty=0.1,
            stop=["END"],
            functions=[custom_function],
            tools=[custom_tool],
            model="gpt-4o",
            response_format={"type": "json_object"},
            seed=42,
            stream=True,
            timeout=30,
            retry_count=5,
            initial_retry_delay=0.5,
            max_retry_delay=10.0,
        )
        assert options.temperature == 0.5
        assert options.max_tokens == 100
        assert options.top_p == 0.9
        assert options.frequency_penalty == 0.2
        assert options.presence_penalty == 0.1
        assert options.stop == ["END"]
        assert options.functions is not None  # Ensure functions is not None
        assert len(options.functions) == 1
        assert options.functions[0].name == "test_function"
        assert options.tools is not None  # Ensure tools is not None
        assert len(options.tools) == 1
        assert options.tools[0].function.name == "test_function"
        assert options.model == "gpt-4o"
        assert options.response_format == {"type": "json_object"}
        assert options.seed == 42
        assert options.stream is True
        assert options.timeout == 30
        assert options.retry_count == 5
        assert options.initial_retry_delay == 0.5
        assert options.max_retry_delay == 10.0

        # Test validation - temperature out of range
        with pytest.raises(ValidationError):
            LLMOptions(temperature=2.5)

        with pytest.raises(ValidationError):
            LLMOptions(temperature=-0.1)

        # Test validation - top_p out of range
        with pytest.raises(ValidationError):
            LLMOptions(top_p=1.5)

        with pytest.raises(ValidationError):
            LLMOptions(top_p=-0.1)

        # Test validation - retry_count negative
        with pytest.raises(ValidationError):
            LLMOptions(retry_count=-1)

    def test_llm_options_to_openai_params(self) -> None:
        """Test converting LLMOptions to OpenAI parameters."""
        # Test with default options
        options = LLMOptions()
        params = options.to_openai_params()

        assert params["temperature"] == 0.7
        assert params["top_p"] == 1.0
        assert params["frequency_penalty"] == 0.0
        assert params["presence_penalty"] == 0.0
        assert "max_tokens" not in params
        assert "stop" not in params
        assert "functions" not in params
        assert "tools" not in params
        assert "response_format" not in params
        assert "seed" not in params
        assert "stream" not in params

        # Test with custom options
        custom_function = FunctionDefinition(name="test_function")
        custom_tool = ToolDefinition(function=FunctionDefinition(name="test_function"))

        options = LLMOptions(
            temperature=0.5,
            max_tokens=100,
            top_p=0.9,
            frequency_penalty=0.2,
            presence_penalty=0.1,
            stop=["END"],
            functions=[custom_function],
            tools=[custom_tool],
            response_format={"type": "json_object"},
            seed=42,
            stream=True,
        )
        params = options.to_openai_params()

        assert params["temperature"] == 0.5
        assert params["max_tokens"] == 100
        assert params["top_p"] == 0.9
        assert params["frequency_penalty"] == 0.2
        assert params["presence_penalty"] == 0.1
        assert params["stop"] == ["END"]
        assert "functions" in params
        assert len(params["functions"]) == 1
        assert params["functions"][0]["name"] == "test_function"
        assert "tools" in params
        assert len(params["tools"]) == 1
        assert params["tools"][0]["function"]["name"] == "test_function"
        assert params["response_format"] == {"type": "json_object"}
        assert params["seed"] == 42
        assert params["stream"] is True

    def test_llm_result(self) -> None:
        """Test the LLMResult model."""
        # Test minimal result
        result = LLMResult(content="Test response", model="gpt-4o")
        assert result.content == "Test response"
        assert result.role == "assistant"
        assert result.model == "gpt-4o"
        assert result.prompt_tokens is None
        assert result.completion_tokens is None
        assert result.total_tokens is None
        assert result.function_call is None
        assert result.tool_calls is None

        # Test complete result
        function_call = FunctionCall(name="test_function", arguments="{}")
        tool_call = ToolCall(
            id="call_1",
            type="function",
            function=FunctionCall(name="test_function", arguments="{}"),
        )

        result = LLMResult(
            content="Test response",
            model="gpt-4o",
            prompt_tokens=10,
            completion_tokens=20,
            total_tokens=30,
            function_call=function_call,
            tool_calls=[tool_call],
        )
        assert result.content == "Test response"
        assert result.role == "assistant"
        assert result.model == "gpt-4o"
        assert result.prompt_tokens == 10
        assert result.completion_tokens == 20
        assert result.total_tokens == 30
        assert result.function_call is not None  # Ensure function_call is not None
        assert result.function_call.name == "test_function"
        assert result.function_call.arguments == "{}"
        assert result.tool_calls is not None  # Ensure tool_calls is not None
        assert len(result.tool_calls) == 1
        assert result.tool_calls[0].id == "call_1"
        assert result.tool_calls[0].function.name == "test_function"

        # Test validation
        with pytest.raises(ValidationError):
            LLMResult(**{"model": "gpt-4o"})  # Missing content

        with pytest.raises(ValidationError):
            LLMResult(**{"content": "Test response"})  # Missing model
